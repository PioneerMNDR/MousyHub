using DocumentFormat.OpenXml.Drawing.Charts;
using LLama.Batched;
using LLama.Common;
using LLama.Native;
using LLama.Sampling;

namespace LLMRP.Components.Models.LLama
{
    public class CustomSampler(Conversation guidance, InferenceParams @params)
         : BaseSamplingPipeline
    {




        protected override LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan<LLamaToken> lastTokens)
        {
            // Get the logits generated by the guidance sequences
            var logits = guidance.Sample();

            {
                if (lastTokens.Length > 0 && (@params.RepeatPenalty != 0f || @params.FrequencyPenalty != 0f || @params.PresencePenalty != 0f))
                {

                    candidates.RepetitionPenalty(ctx, lastTokens, @params.RepeatPenalty, @params.FrequencyPenalty, @params.PresencePenalty);
                }

                // Basic sampling
                candidates.ApplyGrammar(ctx, @params.Grammar);
                candidates.TopK(ctx, @params.TopK, 1uL);
                candidates.LocallyTypical(ctx, @params.TypicalP, 1uL);
                candidates.TopP(ctx, @params.TopP, 1uL);
                candidates.MinP(ctx, @params.MinP, 1uL);
                candidates.Temperature(ctx, @params.Temperature);
                var t = candidates.SampleToken(ctx);
                @params.Grammar?.AcceptToken(ctx, t);
                return t;
            }
        }

        public override void Accept(SafeLLamaContextHandle ctx, LLamaToken token)
        {
        }

        public override ISamplingPipeline Clone()
        {
            throw new NotSupportedException();
        }

        protected override void ProcessLogits(SafeLLamaContextHandle ctx, Span<float> logits, ReadOnlySpan<LLamaToken> lastTokens)
        {
        }
    }
}
